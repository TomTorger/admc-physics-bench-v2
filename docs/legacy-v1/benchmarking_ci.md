# Benchmarking CI Workflow (`bench-and-plot`)

The performance charts in the README are regenerated by the GitHub Actions workflow defined in `.github/workflows/bench.yml`. This note documents what the job does so you can reproduce it locally or adjust the pipeline with confidence.

## Triggers and runner

- **Name:** `bench-and-plot`
- **Triggers:** every push to `main`, plus the manual `workflow_dispatch` hook.
- **Runner:** `ubuntu-latest` (currently Ubuntu 22.04 on an x86_64 virtual machine).

## Toolchain setup

```bash
sudo apt-get update
sudo apt-get install -y cmake ninja-build python3-matplotlib python3-pip
pip3 install --upgrade pip
```

The default GCC toolchain provided by the runner is used (no explicit compiler override). Plotting relies on the system `matplotlib` plus the repo's `tools/plot_perf.py` script.

## Build and bench commands

```bash
cmake -S . -B build -G Ninja -DADMC_BUILD_BENCH=ON -DCMAKE_BUILD_TYPE=Release
cmake --build build --config Release -j

mkdir -p results
for N in 1024 2048 4096 8192; do
  build/bench/bench \
    --scene spheres_cloud --sizes $N \
    --iters 10 --steps 30 \
    --solvers baseline,cached,soa \
    --tile-sizes 64,128,256 \
    --csv results/spheres_cloud_${N}.csv
done
build/bench/bench \
  --scene box_stack_4 --iters 10 --steps 30 \
  --solvers baseline,cached,soa \
  --csv results/box_stack_4.csv
```

> Historical note: CMake produces `build/bench/bench`, then copies it to `bench_main` for backwards compatibility. The workflow now calls the canonical `bench` binary, matching the README instructions.

## Plotting and publishing

```bash
mkdir -p docs/assets
python3 tools/plot_perf.py \
  --inputs results/*.csv \
  --out docs/assets/perf_scaling.svg
```

The plotting script emits both `perf_scaling.svg` and `perf_scaling_speedup.svg`. After plotting the workflow:

1. Commits updated SVGs (and raw CSVs) back to the branch with the message `CI: update perf plots [skip ci]` if changes are detected.
2. Uploads the CSVs and SVGs as an artifact named `bench-results` for inspection.

CSV rows include timing metrics, physics quality metrics, solver identifiers, and (when provided) a `commit_sha`. Downstream tooling can glob `results/*.csv` to reproduce the charts identically.

## Local reproduction checklist

1. Build with the same CMake arguments (`-DADMC_BUILD_BENCH=ON -DCMAKE_BUILD_TYPE=Release`).
2. Run the exact solver/scene loops shown above; they match the CI workload.
3. Call `python3 tools/plot_perf.py --inputs results/*.csv --out docs/assets/perf_scaling.svg` to regenerate both charts.
4. Compare the generated SVGs with `git diff` or overlay them in a browser.
